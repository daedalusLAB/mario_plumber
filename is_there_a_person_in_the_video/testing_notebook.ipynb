{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python\n",
    "# It requires OpenCV installed for Python\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "from sys import platform\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Import openpose libraries\n",
    "sys.path.append(\"/data/home/openpose/build/python\")\n",
    "from openpose import pyopenpose as op\n",
    "\n",
    "import common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenPose Python Wrapper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Error occurred on a thread. OpenPose closed all its threads and then propagated the error to the main thread. Error description:\n",
      "\n",
      "Prototxt file not found: /home/raul/miscosas/openpose/models/pose/body_25/pose_deploy.prototxt.\n",
      "Possible causes:\n",
      "\t1. Not downloading the OpenPose trained models.\n",
      "\t2. Not running OpenPose from the root directory (i.e., where the `model` folder is located, but do not move the `model` folder!). E.g.,\n",
      "\t\tRight example for the Windows portable binary: `cd {OpenPose_root_path}; bin/openpose.exe`\n",
      "\t\tWrong example for the Windows portable binary: `cd {OpenPose_root_path}/bin; openpose.exe`\n",
      "\t3. Using paths with spaces.\n",
      "\n",
      "Coming from:\n",
      "- /data/home/openpose/src/openpose/net/netCaffe.cpp:ImplNetCaffe():61\n",
      "- /data/home/openpose/src/openpose/net/netCaffe.cpp:ImplNetCaffe():97\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractorCaffe.cpp:addCaffeNetOnThread():106\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractorCaffe.cpp:netInitializationOnThread():196\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractorNet.cpp:initializationOnThread():102\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractor.cpp:initializationOnThread():34\n",
      "- /data/home/openpose/include/openpose/pose/wPoseExtractor.hpp:initializationOnThread():57\n",
      "- /data/home/openpose/include/openpose/thread/worker.hpp:initializationOnThreadNoException():77\n",
      "- [All threads closed and control returned to main thread]\n",
      "- /data/home/openpose/src/openpose/utilities/errorAndLog.cpp:checkWorkerErrors():280\n",
      "- /data/home/openpose/include/openpose/thread/threadManager.hpp:stop():243\n",
      "- /data/home/openpose/include/openpose/wrapper/wrapper.hpp:stop():455\n",
      "- /data/home/openpose/include/openpose/wrapper/wrapper.hpp:~WrapperT():282\n",
      "- [Error occurred in a destructor or in the OpenPose Unity Plugin, so no std::exception has been thrown. Returning with exit status 0]\n",
      "\n",
      "Error:\n",
      "Prototxt file not found: /home/raul/miscosas/openpose/models/pose/body_25/pose_deploy.prototxt.\n",
      "Possible causes:\n",
      "\t1. Not downloading the OpenPose t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detecting all available GPUs... Detected 1 GPU(s), using 1 of them starting at GPU 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rained models.\n",
      "\t2. Not running OpenPose from the root directory (i.e., where the `model` folder is located, but do not move the `model` folder!). E.g.,\n",
      "\t\tRight example for the Windows portable binary: `cd {OpenPose_root_path}; bin/openpose.exe`\n",
      "\t\tWrong example for the Windows portable binary: `cd {OpenPose_root_path}/bin; openpose.exe`\n",
      "\t3. Using paths with spaces.\n",
      "\n",
      "Coming from:\n",
      "- /data/home/openpose/src/openpose/net/netCaffe.cpp:ImplNetCaffe():61\n",
      "- /data/home/openpose/src/openpose/net/netCaffe.cpp:ImplNetCaffe():97\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractorCaffe.cpp:addCaffeNetOnThread():106\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractorCaffe.cpp:netInitializationOnThread():196\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractorNet.cpp:initializationOnThread():102\n",
      "- /data/home/openpose/src/openpose/pose/poseExtractor.cpp:initializationOnThread():34\n",
      "- /data/home/openpose/include/openpose/pose/wPoseExtractor.hpp:initializationOnThread():57\n",
      "- /data/home/openpose/include/openpose/thread/worker.hpp:initializationOnThreadNoException():77\n"
     ]
    }
   ],
   "source": [
    "# Starting OpenPose\n",
    "params = dict()\n",
    "params[\"model_folder\"] = \"/home/raul/miscosas/openpose/models/\"\n",
    "openposeWrapper = op.WrapperPython()\n",
    "openposeWrapper.configure(params)\n",
    "openposeWrapper.start()\n",
    "\n",
    "\n",
    "\n",
    "poseModel = op.PoseModel.BODY_25\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = op.Datum()\n",
    "imageToProcess = cv2.imread(\"/data/home/raul/miscosas/openpose/examples/media/COCO_val2014_000000000459.jpg\")\n",
    "datum.cvInputData = imageToProcess\n",
    "openposeWrapper.emplaceAndPop(op.VectorDatum([datum]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nose found\n",
      "Nose found\n"
     ]
    }
   ],
   "source": [
    "if datum.poseKeypoints.size == 0:\n",
    "    print(\"No pose found\")\n",
    "else:\n",
    "  if datum.poseKeypoints.shape[0] > 0:\n",
    "    for i in range(datum.poseKeypoints.shape[0]):\n",
    "      if (datum.poseKeypoints[i][bodyPoints['Nose']] != [0,0,0]).all() and \\\n",
    "          (datum.poseKeypoints[i][bodyPoints['RWrist']] != [0,0,0]).all() and \\\n",
    "                    (datum.poseKeypoints[i][bodyPoints['LWrist']] != [0,0,0]).all():\n",
    "        print(\"Nose found\")\n",
    "\n",
    "(datum.poseKeypoints[0][bodyPoints['Nose']] != [0,0,0]).all()\n",
    "\n",
    "i=0\n",
    "if (datum.poseKeypoints[i][bodyPoints['Nose']] != [0,0,0]).all() and \\\n",
    "          (datum.poseKeypoints[i][bodyPoints['RWrist']] != [0,0,0]).all() and \\\n",
    "          (datum.poseKeypoints[i][bodyPoints['LWrist']] != [0,0,0]).all():\n",
    "          print(\"Nose found\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poses found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-b91bd9df24fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_person_with_hands_in_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/raul/miscosas/openpose/examples/media/COCO_val2014_000000000459.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenposeWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/raul/miscosas/repos/mario_plumber/openpose/common.py\u001b[0m in \u001b[0;36mhave_person_with_hands_in_image\u001b[0;34m(image_path, openposeWrapper)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposeKeypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbodyPoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposeKeypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbodyPoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RWrist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                       \u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposeKeypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbodyPoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LWrist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m           \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nose found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "common.have_person_with_hands_in_image(\"/home/raul/miscosas/openpose/examples/media/COCO_val2014_000000000459.jpg\", openposeWrapper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poses found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d8a31fbed4e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhave_person_with_hands_in_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/raul/miscosas/openpose/examples/media/COCO_val2014_000000000459.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopenposeWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/raul/miscosas/repos/mario_plumber/openpose/common.py\u001b[0m in \u001b[0;36mhave_person_with_hands_in_image\u001b[0;34m(image_path, openposeWrapper)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposeKeypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbodyPoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Nose'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposeKeypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbodyPoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RWrist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                      \u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposeKeypoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbodyPoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LWrist'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m           \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nose found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "common.have_person_with_hands_in_image(\"/home/raul/miscosas/openpose/examples/media/COCO_val2014_000000000459.jpg\", openposeWrapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('whisper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f0143534a4ecc42275807dcd553293e94c0ee080ded93c4d506b74fb48aa96b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
